---
title: "Testing Your Agent"
sidebarTitle: "Testing"
description: "Validate your agent before going live with real callers."
---

Testing is the most important step between building and deploying. A well-tested agent handles real conversations gracefully. A poorly tested one frustrates callers and damages your brand.

Atoms provides multiple testing methods so you can validate thoroughly before going live.

---

## Testing Methods

### Web Call Testing

The fastest way to test. Click **Test Agent** in the top bar to start a browser-based call.

<Frame caption="Test Agent button in the editor">
  <img src="/images/configure.png" alt="Test Agent button" />
</Frame>

**How it works:**
1. Click **Test Agent**
2. Allow microphone access when prompted
3. Talk to your agent like a real caller would
4. The call appears in Conversation Logs when done

**Best for:** Quick iterations, checking prompt changes, initial validation.

### Phone Call Testing

Test with actual phone audio — more realistic than browser testing.

**How it works:**
1. Go to **Settings** → **Test Phone Number**
2. Call the provided number from your phone
3. Have a real conversation with your agent
4. Review the call in Conversation Logs

**Best for:** Final validation, voice quality checks, realistic testing before launch.

---

## What to Test

Don't just test the happy path. Systematically validate every aspect of your agent:

### Conversation Flow

| Test | What You're Checking |
|------|---------------------|
| **Happy path** | Does the ideal conversation work perfectly? |
| **Edge cases** | What happens when callers say unexpected things? |
| **Fallbacks** | Does the agent handle confusion gracefully? |
| **Interruptions** | What happens if you interrupt mid-sentence? |
| **Silence** | What does the agent do when you don't respond? |

### Response Quality

| Test | What You're Checking |
|------|---------------------|
| **Accuracy** | Are answers factually correct? |
| **Relevance** | Does the agent actually answer the question asked? |
| **Tone** | Does it sound like your brand? |
| **Length** | Are responses appropriately concise for voice? |

### Guardrails

| Test | What You're Checking |
|------|---------------------|
| **Off-topic requests** | Does the agent stay on task? |
| **Manipulation attempts** | Can you trick it into breaking rules? |
| **Sensitive topics** | Does it handle them appropriately? |
| **Competitor mentions** | Does it follow your guidelines? |

---

## Testing Checklist

Before going live, verify each of these:

**Basic Functionality**
- [ ] Agent greets correctly
- [ ] Agent understands common requests
- [ ] Agent provides accurate information
- [ ] Agent ends calls gracefully

**Edge Cases**
- [ ] Agent handles unclear speech
- [ ] Agent manages interruptions
- [ ] Agent recovers from silence
- [ ] Agent deals with unexpected topics

**Integrations** (if applicable)
- [ ] API calls work correctly
- [ ] Data is fetched/sent properly
- [ ] Transfer calls connect
- [ ] Variables populate correctly

**Quality**
- [ ] Voice sounds natural
- [ ] Response timing feels right
- [ ] Personality matches brand
- [ ] No awkward pauses

---

## Testing Tips

<AccordionGroup>
  <Accordion title="Test like your actual callers">
    Don't use perfect English. Mumble. Use filler words. Ask things in weird ways. Real callers don't speak like prompts.
  </Accordion>
  <Accordion title="Try to break it">
    Ask off-topic questions. Try to get it to reveal information it shouldn't. Interrupt constantly. Your callers will find edge cases — find them first.
  </Accordion>
  <Accordion title="Test in real conditions">
    Background noise, poor phone connection, speaking quickly. Test the conditions your real callers will have.
  </Accordion>
  <Accordion title="Have others test">
    You know your agent too well. Fresh testers find problems you miss because they don't know the "right" way to talk to it.
  </Accordion>
  <Accordion title="Review every log">
    After testing, check Conversation Logs. Read the transcript. Listen to the audio. Look for anywhere the agent could have done better.
  </Accordion>
</AccordionGroup>

---

## Interpreting Test Results

After each test call, check:

**Transcript Accuracy**
Was what you said transcribed correctly? Speech recognition errors can cause downstream problems.

**Intent Understanding**
Did the agent understand what you meant, even if you said it imperfectly?

**Response Appropriateness**
Was the response helpful, on-topic, and correctly toned?

**Flow Progression**
Did the conversation move forward naturally, or did it get stuck or repeat?

---

## Iterating on Problems

When tests reveal issues:

1. **Identify the pattern** — Is it a one-off or does it happen consistently?
2. **Check the logs** — What exactly did the agent see and decide?
3. **Trace to the cause** — Is it a prompt issue, LLM issue, or integration issue?
4. **Make targeted fixes** — Change one thing at a time so you know what worked
5. **Re-test the specific scenario** — Confirm the fix works
6. **Re-test adjacent scenarios** — Make sure you didn't break something else

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Conversation Logs" icon="scroll" href="/platform/building-agents/testing-management/conversation-logs">
    Analyze what happened in test calls
  </Card>
  <Card title="Locking Your Agent" icon="lock" href="/platform/building-agents/testing-management/locking/when-to-lock">
    Freeze your agent when it's ready
  </Card>
</CardGroup>
